{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Movie Recommendation System using SageMaker and Factorization Machines\n",
    "\n",
    "In this workshop you will be building a Movie Recommendation System using Factorization Machines. You will learn about the capabilities of SageMaker in building, training, and deploying models at scale. You will fast track many of the troublesome tasks required through productionising your model and focus more on the actual development and value. But before we get started, let's have a short introduction on Factorization Machines. Also, ask us questions as many times as you wish, we won't bite.\n",
    "\n",
    "## Factorization Machines \n",
    "Factorization Machines is one of the new craze in the in the supervised learning algorithm world. It is built for both classification and regression problems as it is an extension of the linear model. One of the key benefits behind Factorization Machines is its ability to deal with high dimensional sparse data (which is an awesome example of a sparse matrix of user ratings and movies).\n",
    "\n",
    "If you wish to learn more about Factorizatin Machines, there are plenty of mathematical resources out there and you will be surprise how simple the whole thing is... \n",
    "\n",
    "# Let's get started!\n",
    "We will first create a username for yourself which will be used to run the website and get your recommendations and not others. Create your username as \"{your_initial}_{4 random numbers}\" see example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"jy_1234\" < example\n",
    "# username = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started!\n",
    "First we will load a few packages that we require for general purpose and those required by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import lil_matrix\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "import s3fs #!pip install s3fs\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Getting and Setting Permission\n",
    "In order to use SageMaker you will need to initiate a few sesions and get your role. We will be initiating the SageMaker session when we train our model but before that we will need to call the s3 resource which we will use to upload our file to our s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "current_region = boto3.Session().region_name\n",
    "s3 = boto3.resource('s3')\n",
    "sm_sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data\n",
    "In this workshop we will be using the 100k MovieLens dataset. The MovieLens data was collected by the Grouplens Research Project, from the MovieLens website. The data contains users (and their demographic information), movies and the ratings for each movies provided by each users.\n",
    "\n",
    "Summary:\n",
    "* ~100,000 ratings made by 943 users\n",
    "* Each user has rated at least 20 movies\n",
    "\n",
    "From the 100k Dataset, we will be using the ua.base and ua.test tab delimited data. The ua.base data will be used as our training set while the ua.test will be used as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'aimlws001s3-dataset'\n",
    "\n",
    "# user movie ratings data\n",
    "umr_train_key = 'Dataset/ml-100k/ua.base'\n",
    "umr_train_location = 's3://{}/{}'.format(bucket, umr_train_key) \n",
    "\n",
    "umr_test_key = 'Dataset/ml-100k/ua.test'\n",
    "umr_test_location = 's3://{}/{}'.format(bucket, umr_test_key) \n",
    "\n",
    "# Load Training Set #\n",
    "umr_train = pd.read_csv(\n",
    "    umr_train_location, \n",
    "    sep = '\\t',\n",
    "    dtype={'userId':'int32', 'movieId':'int32', 'rating':'float32'},\n",
    "    names = ['user_id' , 'movie_id' , 'rating'], \n",
    "    index_col = False\n",
    ")\n",
    "umr_train = shuffle(umr_train) # shuffle data\n",
    "\n",
    "# Load Test Set\n",
    "umr_test = pd.read_csv(\n",
    "    umr_test_location, \n",
    "    sep = '\\t',\n",
    "    dtype={'userId':'int32', 'movieId':'int32', 'rating':'float32'},\n",
    "    names = ['user_id' , 'movie_id' , 'rating'], \n",
    "    index_col = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity: Create your own rating, and pick a movie using the final_movie_metadata, appendend that to umr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = [1, 2, 3] # change this\n",
    "rating= [1.0, 2.0, 3.0] # change this\n",
    "user_id = [944] * len(rating) # don't change this\n",
    "my_data = pd.DataFrame({\"user_id\": user_id, \"movie_id\": movie_id, \"rating\": rating})\n",
    "umr_train = my_data.append(umr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Explore the data!\n",
    "\n",
    "We can see that there are ~90k ratings in ua.base and ~1k ratings in ua.test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users_train = umr_train['user_id'].max()\n",
    "nb_movies_train = umr_train['movie_id'].max()\n",
    "nb_ratings_train = umr_train.shape[0]\n",
    "nb_features = nb_users_train + nb_movies_train\n",
    "\n",
    "print(\"Number of users: \", nb_users_train)\n",
    "print(\"Number of movies: \", nb_movies_train)\n",
    "print(\"Number of ratings: \", nb_ratings_train)\n",
    "print(\"Number of features: \", nb_features)\n",
    "umr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users_test = umr_test['user_id'].max()\n",
    "nb_movies_test = umr_test['movie_id'].max()\n",
    "nb_ratings_test = umr_test.shape[0]\n",
    "\n",
    "print(\"Number of users: \", nb_users_test)\n",
    "print(\"Number of movies: \", nb_movies_test)\n",
    "print(\"Number of ratings: \", nb_ratings_test)\n",
    "umr_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activities:\n",
    "\n",
    "1. Find the most popular movie, the one with the highest number of ratings.\n",
    "\n",
    "2. Find the user who has watched the most movies and how many times have they rated 1, 2 ... 5 etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Transforming the data into protobuf\n",
    "### Create one-hot encoded sparse matrix\n",
    "SM Factorization Machine requires that the data be in RecordIO-protobuf format with a Float32 tensor. Luckily, we don't have to build our own utilities function as SageMaker can easily help us out with this through their SDK.\n",
    "\n",
    "However, we will first create one-hot encoded sparse matrix. Since FM is a binary classifier, any movies with >4 rating score will be assigned with 1, else 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(dataframe, lines, nb_users, columns):\n",
    "    # Create sparse matrix of one-hot encoded features\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    Y = [] # Store labels in a vector\n",
    "    \n",
    "    line = 0\n",
    "    for index, row in dataframe.iterrows():\n",
    "        X[line, row['user_id'] - 1] = 1\n",
    "        X[line, nb_users + (row['movie_id'] - 1)] = 1\n",
    "            \n",
    "        if int(row['rating']) >= 4:\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "            \n",
    "        line = line+1\n",
    "\n",
    "    Y = np.array(Y).astype('float32')            \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_sparse_matrix(umr_train, nb_ratings_train, nb_users_train, nb_features)\n",
    "X_test, Y_test = create_sparse_matrix(umr_test, nb_ratings_test, nb_users_test, nb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write sparse matrix as protobuf to S3 Bucket\n",
    "We will now write our sparse matrix as protobuf and upload that to s3. be sure you use your own folder, your <i>name</i> as a prefix to <i>_pf_train_test</i> is fine.\n",
    "\n",
    "The data files, training and test protobuf sets, will be saved in a training and test folder respectively within the 'aimlws001s3-dataset' bucket. You will also be setting the path of where your model output (or artifact) will be saved which will likely be in aimlws001s3-dataset bucket within output folder in your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_matrix_protobuf_s3(X, bucket, prefix, key, Y=None):\n",
    "    \n",
    "    buffer = io.BytesIO()\n",
    "    \n",
    "    smac.write_spmatrix_to_sparse_tensor(buffer, X, labels=Y)\n",
    "        \n",
    "    buffer.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    \n",
    "    uploaded_path = 's3://{}/{}'.format(bucket, obj)\n",
    "    \n",
    "    s3.Bucket(bucket).Object(obj).upload_fileobj(buffer)\n",
    "    \n",
    "    return (uploaded_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_folder_name = '{}/pf_train_test'.format(username)\n",
    "\n",
    "umr_train_key = 'training/umr.train.protobuf'\n",
    "umr_test_key = 'test/umr.test.protobuf'\n",
    "\n",
    "model_output_path = 's3://{}/{}/output'.format(bucket, your_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protobuf_path = write_matrix_protobuf_s3(X_train, bucket, your_folder_name, umr_train_key, Y_train)    \n",
    "test_protobuf_path  = write_matrix_protobuf_s3(X_test, bucket, your_folder_name, umr_test_key, Y_test)    \n",
    "\n",
    "print('Location of your protobuf training set: ', train_protobuf_path)\n",
    "print('Location of your protobuf test set: ', test_protobuf_path)\n",
    "print('Location of your model output: ', model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SageMaker Factorization Machines Model\n",
    "\n",
    "#### Algorithm Image Uri\n",
    "\n",
    "In order to use factoziation-machines as our algorithm, we need to grab the container that holds that amazon algorithm first. To do so, we specify the region and the algorithm to get the amazong image uri. \n",
    "\n",
    "#### Output Path \n",
    "\n",
    "Our SageMaker will output the model's artifact in the path that we have set above. \n",
    "\n",
    "#### Instance\n",
    "\n",
    "The key thing about SageMaker is that you will only pay for what you use in training. You can specify the instance type (compute power/memory power) you want to use to train and build your model. We will default this to ml.m5.large for now. Once it's done training, you will not be charged, pay for what you use (per second billable model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'factorization-machines'\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    get_image_uri(current_region, algorithm),\n",
    "    role, \n",
    "    train_instance_count = 1, \n",
    "    train_instance_type = 'ml.m5.large',\n",
    "    output_path = model_output_path,\n",
    "    sagemaker_session = sm_sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play around with the hyperparameters of the factorization machines. This is used to fine tune your model to achieve a better accuracy. For the time being we will default the mini-batch size and number of epochs for our first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.set_hyperparameters(\n",
    "    feature_dim = nb_features,\n",
    "    predictor_type = 'binary_classifier',\n",
    "    num_factors = 64,\n",
    "    mini_batch_size = 1000,\n",
    "    epochs = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model you just simply call SageMaker fit and associate it with the path of your training data. Optional, you can also add the test set to give you the accuracy score as it trains your model and evaluates it against the test set - we will do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.fit({\n",
    "    'train': train_protobuf_path,  \n",
    "    'test': test_protobuf_path\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy in the experiments section\n",
    "Try Changing the Hyperparameters to see if you get a better accuracy. You can view all your trained model in experiments. Just uncomment the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fm.set_hyperparameters(\n",
    "#     feature_dim = nb_features,\n",
    "#     predictor_type = 'binary_classifier',\n",
    "#     num_factors = 64,\n",
    "#     mini_batch_size = # change,\n",
    "#     epochs = # change here\n",
    "#     # add more if you wish\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model\n",
    "\n",
    "Once you're happy with your accuracy, you can deploy the model using the command below. We will be using ml.c5.xlarge to host the model. You can scale it according to your need but we can just stick to 1 instance for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(initial_instance_count = 1,\n",
    "                         instance_type = 'ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is your endpoint, make note of this: \", fm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Before we make predictions, we need to make sure we build some functions to serialize and deserialize the model as this is what SageMaker FM requires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.predictor import json_deserializer\n",
    "def fm_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        js['instances'].append({'features': row.tolist()})\n",
    "    return json.dumps(js)\n",
    "\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prediction using the endpoint to find your movie recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "user_id = 944 # leave this as it is, this is your unique userid\n",
    "for i in range (nb_users_train + 1, nb_features): \n",
    "    X_new = lil_matrix((1, nb_features)).astype('float32')\n",
    "    X_new[0, user_id] = 1\n",
    "    X_new[0, i] = 1\n",
    "    \n",
    "    pred = fm_predictor.predict(X_new[0].toarray())[\"predictions\"][0]\n",
    "    pred[\"movie_id\"] = i - nb_users_train\n",
    "    predictions.append(pred)\n",
    "top_n_predictions = sorted(predictions, key = lambda i: i['score'], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_movie_id = []\n",
    "for x in top_n_predictions[0:44]:\n",
    "    list_movie_id.append(x[\"movie_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take the top 45 movies and load it into Servianflix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_load = pd.read_csv(\n",
    "    \"final_movie_metadata.csv\", \n",
    "    sep = ',',\n",
    "    index_col = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('movie_md')\n",
    "\n",
    "for index, row in movies_load.iterrows():\n",
    "\n",
    "    if row[\"movieId\"] in list_movie_id:\n",
    "        \n",
    "        my_item = {\n",
    "            'movieId': str(row[\"movieId\"]),\n",
    "            'moviePoster': str(row[\"moviePoster\"]),\n",
    "            'movieTitle': str(row[\"movieTitle\"]),\n",
    "            'movieReleaseYear': str(row[\"movieReleaseYear\"]),\n",
    "            'genre': username,\n",
    "            'overview': str(row['overview']),\n",
    "            'vote_average': str(row['vote_average']),\n",
    "            'vote_count': str(row['vote_count'])\n",
    "        }\n",
    "\n",
    "        response = table.put_item(Item=my_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Delete Endpoint\n",
    "\n",
    "Run this after you're done with everything so you don't have an outgoing cost, but if you want to be evil and burn through our beer funds, let the endpoint running ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker.Session().delete_endpoint(fm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Thanks to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to AWS - specifically Julia Simon, Zohar Karnin, Rama Thamman, Sireesha Muppala, Yuri Astashanok, David Arpin, and Guy Ernest for creating many of the baseline functionalities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
